{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"name":"korean-mrc-baseline-goorm-4.ipynb","provenance":[]}},"nbformat_minor":0,"nbformat":4,"cells":[{"cell_type":"markdown","source":["##GitHub 연동"],"metadata":{"id":"mejIU_ritf1r"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PVLIECjci_4z","executionInfo":{"status":"ok","timestamp":1658305628865,"user_tz":-540,"elapsed":20661,"user":{"displayName":"이주혁","userId":"02129413503939756678"}},"outputId":"ddad58cf-1a75-4699-f013-148a770df918"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["\n","# 클론시 (개인)\n","#!git clone https://Leejuhyuk26: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@github.com/Leejuhyuk26/goorm_second_project.git"],"metadata":{"id":"PrqktkqCjain"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/goorm_project/goorm_second_project"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c2UUfyO2jJ7V","executionInfo":{"status":"ok","timestamp":1658305633540,"user_tz":-540,"elapsed":345,"user":{"displayName":"이주혁","userId":"02129413503939756678"}},"outputId":"fd42125a-e840-4729-c70a-3c23e5edce6e"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/goorm_project/goorm_second_project\n"]}]},{"cell_type":"code","source":["!pwd"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I1yTsFpPjPCi","executionInfo":{"status":"ok","timestamp":1658305636740,"user_tz":-540,"elapsed":443,"user":{"displayName":"이주혁","userId":"02129413503939756678"}},"outputId":"02e137fe-5e5d-4cb4-ddaf-77312f01923d"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/goorm_project/goorm_second_project\n"]}]},{"cell_type":"code","source":["!git config --global user.email 'gbdldl@naver.com'\n","!git config --global user.name 'Leejuhyuk26'"],"metadata":{"id":"4cWwRZ8WjRJt","executionInfo":{"status":"ok","timestamp":1658305640962,"user_tz":-540,"elapsed":1463,"user":{"displayName":"이주혁","userId":"02129413503939756678"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["!git add korean-mrc-baseline-goorm-4.ipynb"],"metadata":{"id":"zk93L9L-jjUy","executionInfo":{"status":"ok","timestamp":1658305645664,"user_tz":-540,"elapsed":1860,"user":{"displayName":"이주혁","userId":"02129413503939756678"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["!git commit -m '20220720 test commit try 4'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fty3SfN9uBbu","executionInfo":{"status":"ok","timestamp":1658305663122,"user_tz":-540,"elapsed":3425,"user":{"displayName":"이주혁","userId":"02129413503939756678"}},"outputId":"635242fa-bc62-43c4-d912-04774e077019"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["[main 48723cb] 20220720 test commit try 4\n"," 1 file changed, 1 insertion(+), 1 deletion(-)\n"," rewrite korean-mrc-baseline-goorm-4.ipynb (95%)\n"]}]},{"cell_type":"code","source":["!git push origin master"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3OygAS34uFcR","executionInfo":{"status":"ok","timestamp":1658305687544,"user_tz":-540,"elapsed":762,"user":{"displayName":"이주혁","userId":"02129413503939756678"}},"outputId":"39a1fe96-67e5-42c7-b827-4dbc9f070cbc"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["error: src refspec master does not match any.\n","error: failed to push some refs to 'https://Leejuhyuk26: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@github.com/Leejuhyuk26/goorm_second_project.git'\n"]}]},{"cell_type":"code","source":["git init\n","git add .\n","git commit -m \"init\"\n","git remote add origin [Github repository Address]\n","git push -u origin main"],"metadata":{"id":"KWJf-McmudMC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Korean MRC Baseline\n","\n","## Dependency\n","다음과 같은 라이브러리를 사용한다.\n","- [Konlpy](https://konlpy.org/ko/latest/index.html): 파이썬 한국어 NLP 처리기\n","- [Mecab-korean](https://bitbucket.org/eunjeon/mecab-ko-dic/src): 한국어 형태소 분석기"],"metadata":{"id":"4E47uJwWXMwj"}},{"cell_type":"code","source":["! apt-get install -y openjdk-8-jdk python3-dev\n","! pip install konlpy \"tweepy<4.0.0\"\n","! /bin/bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)"],"metadata":{"trusted":true,"id":"A_zBXD5YXMw8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 데이터셋 구성\n","현재 JSON 데이터를 볼 수 있는 클래스를 하나 작성하자."],"metadata":{"id":"Vbyl3O36XMxC"}},{"cell_type":"code","source":["from typing import List, Tuple, Dict, Any\n","import json\n","import random\n","\n","class KoMRC:\n","    def __init__(self, data, indices: List[Tuple[int, int, int]]):\n","        self._data = data\n","        self._indices = indices\n","\n","    # Json을 불러오는 메소드\n","    @classmethod\n","    def load(cls, file_path: str):\n","        with open(file_path, 'r', encoding='utf-8') as fd:\n","            data = json.load(fd)\n","\n","        indices = []\n","        for d_id, document in enumerate(data['data']):\n","            for p_id, paragraph in enumerate(document['paragraphs']):\n","                for q_id, _ in enumerate(paragraph['qas']):\n","                    indices.append((d_id, p_id, q_id))\n","        \n","        return cls(data, indices)\n","\n","    # 데이터 셋을 잘라내는 메소드\n","    @classmethod\n","    def split(cls, dataset, eval_ratio: float=.1, seed=42):\n","        indices = list(dataset._indices)\n","        random.seed(seed)\n","        random.shuffle(indices)\n","        train_indices = indices[int(len(indices) * eval_ratio):]\n","        eval_indices = indices[:int(len(indices) * eval_ratio)]\n","\n","        return cls(dataset._data, train_indices), cls(dataset._data, eval_indices)\n","\n","    def __getitem__(self, index: int) -> Dict[str, Any]:\n","        d_id, p_id, q_id = self._indices[index]\n","        paragraph = self._data['data'][d_id]['paragraphs'][p_id]\n","\n","        context = paragraph['context']\n","        qa = paragraph['qas'][q_id]\n","\n","        guid = qa['guid']\n","        question = qa['question']\n","        answers = qa['answers']\n","\n","        return {\n","            'guid': guid,\n","            'context': context,\n","            'question': question,\n","            'answers': answers\n","        }\n","\n","    def __len__(self) -> int:\n","        return len(self._indices)\n"],"metadata":{"trusted":true,"id":"MYI4hPqHXMxH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["`load` 메소드를 이용해서 Json 데이터를 불러올 수 있다."],"metadata":{"id":"ZRBCE1FgXMxP"}},{"cell_type":"code","source":["dataset = KoMRC.load('/kaggle/input/k-digital-goorm-4-korean-mrc/train.json')\n","print(\"Number of Samples:\", len(dataset))\n","print(dataset[0])"],"metadata":{"trusted":true,"id":"dhcVt4eOXMxS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["`split` 메소드를 이용하면 데이터 셋을 나눌 수 있다."],"metadata":{"id":"BtfYINC-XMxU"}},{"cell_type":"code","source":["train_dataset, dev_dataset = KoMRC.split(dataset)\n","print(\"Number of Train Samples:\", len(train_dataset))\n","print(\"Number of Dev Samples:\", len(dev_dataset))\n","print(dev_dataset[0])"],"metadata":{"trusted":true,"id":"Xs3Oo8OTXMxj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["단어 단위로 토큰화해서 정답 위치를 찾기 위하여 토큰화 및 위치 인덱싱을 하는 클래스를 상속을 통해 작성해 보자."],"metadata":{"id":"c37YC5bXXMxo"}},{"cell_type":"code","source":["from typing import Generator\n","\n","import konlpy\n","\n","class TokenizedKoMRC(KoMRC):\n","    def __init__(self, data, indices: List[Tuple[int, int, int]]) -> None:\n","        super().__init__(data, indices)\n","        self._tagger = konlpy.tag.Mecab()\n","\n","    def _tokenize_with_position(self, sentence: str) -> List[Tuple[str, Tuple[int, int]]]:\n","        position = 0\n","        tokens = []\n","        for morph in self._tagger.morphs(sentence):\n","            position = sentence.find(morph, position)\n","            tokens.append((morph, (position, position + len(morph))))\n","            position += len(morph)\n","        return tokens\n","            \n","    def __getitem__(self, index: int) -> Dict[str, Any]:\n","        sample = super().__getitem__(index)\n","\n","        context, position = zip(*self._tokenize_with_position(sample['context']))\n","        context, position = list(context), list(position)\n","        question = self._tagger.morphs(sample['question'])\n","\n","        if sample['answers'] is not None:\n","            answers = []\n","            for answer in sample['answers']:\n","                for start, (position_start, position_end) in enumerate(position):\n","                    if position_start <= answer['answer_start'] < position_end:\n","                        break\n","                else:\n","                    print(context, answer)\n","                    raise ValueError(\"No mathced start position\")\n","\n","                target = ''.join(answer['text'].split(' '))\n","                source = ''\n","                for end, morph in enumerate(context[start:], start):\n","                    source += morph\n","                    if target in source:\n","                        break\n","                else:\n","                    print(context, answer)\n","                    raise ValueError(\"No Matched end position\")\n","\n","                answers.append({\n","                    'start': start,\n","                    'end': end\n","                })\n","        else:\n","            answers = None\n","        \n","        return {\n","            'guid': sample['guid'],\n","            'context_original': sample['context'],\n","            'context_position': position,\n","            'question_original': sample['question'],\n","            'context': context,\n","            'question': question,\n","            'answers': answers\n","        }\n","        "],"metadata":{"trusted":true,"id":"GpyR6vBeXMxt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["mecabtest = konlpy.tag.Mecab()\n","\n","mecabtest.pos(\"한국어 테스트\")\n","#mecabtest.nouns(\"한국어 테스트\")\n","#공백이 날아가는 문제점"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eK5kjHaHrYC8","executionInfo":{"status":"ok","timestamp":1658294901762,"user_tz":-540,"elapsed":343,"user":{"displayName":"이주혁","userId":"02129413503939756678"}},"outputId":"3d7e16aa-d5d7-4fdd-e731-b4b0980b0bbf"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('한국어', 'NNG'), ('테스트', 'NNG')]"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["dataset = TokenizedKoMRC.load('/kaggle/input/k-digital-goorm-4-korean-mrc/train.json')\n","\n","train_dataset, dev_dataset = TokenizedKoMRC.split(dataset)\n","print(\"Number of Train Samples:\", len(train_dataset))\n","print(\"Number of Dev Samples:\", len(dev_dataset))\n","print(dev_dataset[0])"],"metadata":{"trusted":true,"id":"Ja3aa_kCXMx0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sample = dev_dataset[0]\n","print(sample['context'][sample['answers'][0]['start']:sample['answers'][0]['end']+1])"],"metadata":{"trusted":true,"id":"W2k5E8dCXMx2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Vocab 생성 및 Indexing\n","토큰화된 데이터 셋을 기준으로 Vocab을 만들고 인덱싱을 하는 `Indexer`를 만들자."],"metadata":{"id":"dDyBJVoTXMx7"}},{"cell_type":"code","source":["from typing import Sequence\n","from collections import Counter\n","from itertools import chain\n","\n","from tqdm.notebook import tqdm\n","\n","class Indexer:\n","    def __init__(self,\n","        id2token: List[str], \n","        max_length: int=1024,\n","        pad: str='<pad>', unk: str='<unk>', cls: str='<cls>', sep: str='<sep>'\n","    ):\n","        self.pad = pad\n","        self.unk = unk\n","        self.cls = cls\n","        self.sep = sep\n","        self.special_tokens = [pad, unk, cls, sep]\n","\n","        self.max_length = max_length\n","\n","        self.id2token = self.special_tokens + id2token\n","        self.token2id = {token: token_id for token_id, token in enumerate(self.id2token)}\n","\n","    @property\n","    def vocab_size(self):\n","        return len(self.id2token)\n","    \n","    @property\n","    def pad_id(self):\n","        return self.token2id[self.pad]\n","    @property\n","    def unk_id(self):\n","        return self.token2id[self.unk]\n","    @property\n","    def cls_id(self):\n","        return self.token2id[self.cls]\n","    @property\n","    def sep_id(self):\n","        return self.token2id[self.sep]\n","\n","    @classmethod\n","    def build_vocab(cls,\n","        dataset: TokenizedKoMRC, \n","        min_freq: int=5\n","    ):\n","        counter = Counter(chain.from_iterable(\n","            sample['context'] + sample['question']\n","            for sample in tqdm(dataset, desc=\"Counting Vocab\")\n","        ))\n","\n","        return cls([word for word, count in counter.items() if count >= min_freq])\n","    \n","    def decode(self,\n","        token_ids: Sequence[int]\n","    ):\n","        return [self.id2token[token_id] for token_id in token_ids]\n","\n","    def sample2ids(self,\n","        sample: Dict[str, Any],\n","    ) -> Dict[str, Any]:\n","        context = [self.token2id.get(token, self.unk_id) for token in sample['context']]\n","        question = [self.token2id.get(token, self.unk_id) for token in sample['question']]\n","\n","        context = context[:self.max_length-len(question)-3]             # Truncate context\n","        \n","        input_ids = [self.cls_id] + question + [self.sep_id] + context + [self.sep_id]\n","        token_type_ids = [0] * (len(question) + 1) + [1] * (len(context) + 2)\n","\n","        if sample['answers'] is not None:\n","            answer = sample['answers'][0]\n","            start = min(answer['start'] + len(question) + 2, self.max_length - 1)\n","            end = min(answer['end'] + len(question) + 2, self.max_length - 1)\n","        else:\n","            start = None\n","            end = None\n","\n","        return {\n","            'guid': sample['guid'],\n","            'context': sample['context_original'],\n","            'question': sample['question_original'],\n","            'position': sample['context_position'],\n","            'input_ids': input_ids,\n","            'token_type_ids': token_type_ids,\n","            'start': start,\n","            'end': end\n","        }"],"metadata":{"trusted":true,"id":"r2Yk13M5XMx-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["indexer = Indexer.build_vocab(dataset)\n","print(indexer.sample2ids(dev_dataset[0]))"],"metadata":{"trusted":true,"id":"zKX5b0GRXMyA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["쉽게 Indexer를 활용하기 위해 Indexer가 포함된 데이터 셋을 만들자."],"metadata":{"id":"n_23Sw_vXMyC"}},{"cell_type":"code","source":["class IndexerWrappedDataset:\n","    def __init__(self, dataset: TokenizedKoMRC, indexer: Indexer) -> None:\n","        self._dataset = dataset\n","        self._indexer = indexer\n","\n","    def __len__(self) -> int:\n","        return len(self._dataset)\n","    \n","    def __getitem__(self, index: int) -> Dict[str, Any]:\n","        sample = self._indexer.sample2ids(self._dataset[index])\n","        sample['attention_mask'] = [1] * len(sample['input_ids'])\n","\n","        return sample\n"],"metadata":{"trusted":true,"id":"yRPv1k7vXMyF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["indexed_train_dataset = IndexerWrappedDataset(train_dataset, indexer)\n","indexed_dev_dataset = IndexerWrappedDataset(dev_dataset, indexer)\n","\n","sample = indexed_dev_dataset[0]\n","print(sample['input_ids'], sample['attention_mask'], sample['token_type_ids'], sample['start'], sample['end'])"],"metadata":{"trusted":true,"id":"QZoHzrJYXMyG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Transformer Encoder를 활용한 MRC 모델\n","![Bert for MRC](https://miro.medium.com/max/340/1*cXDOP0gsE7Zp8-sgZqYfTA.png)\n","\n","Transformer 인코더 마지막에 Linear Layer를 붙여 정답의 시작과 끝을 맞추는 간단한 모델을 생성보자."],"metadata":{"id":"WW9HYo0WXMyI"}},{"cell_type":"code","source":["import torch.nn as nn\n","\n","from transformers.models.bert.modeling_bert import (\n","    BertModel,\n","    BertPreTrainedModel\n",")\n","\n","## Simple Version for Bert QA: https://huggingface.co/transformers/_modules/transformers/models/bert/modeling_bert.html#BertForQuestionAnswering.forward\n","class BertForQuestionAnswering(BertPreTrainedModel):\n","    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n","\n","    def __init__(self, config):\n","        super().__init__(config)\n","        self.bert = BertModel(config, add_pooling_layer=False)\n","        self.start_linear = nn.Linear(config.hidden_size, 1)\n","        self.end_linear = nn.Linear(config.hidden_size, 1)\n","\n","        self.init_weights()\n","\n","    def forward(\n","        self,\n","        input_ids=None,\n","        attention_mask=None,\n","        token_type_ids=None\n","    ):\n","        outputs = self.bert(\n","            input_ids,\n","            attention_mask=attention_mask,\n","            token_type_ids=token_type_ids,\n","        )\n","\n","        start_logits = self.start_linear(outputs.last_hidden_state).squeeze(-1)\n","        end_logits = self.end_linear(outputs.last_hidden_state).squeeze(-1)\n","\n","        return start_logits, end_logits"],"metadata":{"trusted":true,"id":"El9geLUIXMyL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 학습 준비"],"metadata":{"id":"2vcsQp2vXMyN"}},{"cell_type":"code","source":["import torch\n","from torch.nn.utils.rnn import pad_sequence\n","\n","class Collator:\n","    def __init__(self, indexer: Indexer) -> None:\n","        self._indexer = indexer\n","\n","    def __call__(self, samples: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:\n","        samples = {\n","            key: [sample[key] for sample in samples]\n","            for key in samples[0]\n","        }\n","\n","        for key in 'start', 'end':\n","            if samples[key][0] is None:\n","                samples[key] = None\n","            else:\n","                samples[key] = torch.tensor(samples[key], dtype=torch.long)\n","        for key in 'input_ids', 'attention_mask', 'token_type_ids':\n","            samples[key] = pad_sequence(\n","                [torch.tensor(sample, dtype=torch.long) for sample in samples[key]],\n","                batch_first=True, padding_value=self._indexer.pad_id\n","            )\n","\n","        return samples"],"metadata":{"trusted":true,"id":"yVgDaU73XMyP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch.utils.data import DataLoader\n","\n","batch_size = 64\n","accumulation = 4 # 메모리를 아끼기 위하여 Gradient accumulation을 해보자\n","\n","collator = Collator(indexer)\n","train_loader = DataLoader(indexed_train_dataset, batch_size=batch_size//accumulation, shuffle=True, collate_fn=collator, num_workers=2)\n","dev_loader = DataLoader(indexed_dev_dataset, batch_size=batch_size//accumulation, shuffle=False, collate_fn=collator, num_workers=2)"],"metadata":{"trusted":true,"id":"l7WH2917XMyQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["batch = next(iter(dev_loader))\n","print(batch['input_ids'].shape)\n","print(batch['input_ids'])\n","print(list(batch.keys()))"],"metadata":{"trusted":true,"id":"vWNMCfyrXMyS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","from transformers import BertConfig\n","\n","torch.manual_seed(42)\n","config = BertConfig(\n","     vocab_size=indexer.vocab_size,\n","     max_position_embeddings=1024,\n","     hidden_size=256,\n","     num_hidden_layers=4,\n","     num_attention_heads=4,\n","     intermediate_size=1024\n",")\n","model = BertForQuestionAnswering(config)\n","# model.cuda()\n","optimizer = torch.optim.AdamW(model.parameters(), lr=2e-4)"],"metadata":{"trusted":true,"id":"XBw0Ev0BXMyT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","from statistics import mean\n","\n","import torch.nn.functional as F\n","from torch.nn.utils import clip_grad_norm_\n","\n","os.makedirs('dump', exist_ok=True)\n","train_losses = []\n","dev_losses = []\n","\n","step = 0\n","\n","for epoch in range(1, 31):\n","    print(\"Epoch\", epoch)\n","    # Training\n","    running_loss = 0.\n","    losses = []\n","    progress_bar = tqdm(train_loader, desc='Train')\n","    for batch in progress_bar:\n","        del batch['guid'], batch['context'], batch['question'], batch['position']\n","        # batch = {key: value.cuda() for key, value in batch.items()}\n","        start = batch.pop('start')\n","        end = batch.pop('end')\n","        \n","        start_logits, end_logits = model(**batch)\n","        loss = F.cross_entropy(start_logits, start) + F.cross_entropy(end_logits, end)\n","        (loss / accumulation).backward()\n","        running_loss += loss.item()\n","        del batch, start, end, start_logits, end_logits, loss\n","        \n","        step += 1\n","        if step % accumulation:\n","            continue\n","\n","        clip_grad_norm_(model.parameters(), max_norm=1.)\n","        optimizer.step()\n","        optimizer.zero_grad(set_to_none=True)\n","\n","        losses.append(running_loss / accumulation)\n","        running_loss = 0.\n","        progress_bar.set_description(f\"Train - Loss: {losses[-1]:.3f}\")\n","    train_losses.append(mean(losses))\n","    print(f\"train score: {train_losses[-1]:.3f}\")\n","\n","    # Evaluation\n","    losses = []\n","    for batch in tqdm(dev_loader, desc=\"Evaluation\"):\n","        del batch['guid'], batch['context'], batch['question'], batch['position']\n","        # batch = {key: value.cuda() for key, value in batch.items()}\n","        start = batch.pop('start')\n","        end = batch.pop('end')\n","        \n","        with torch.no_grad():\n","            start_logits, end_logits = model(**batch)\n","        loss = F.cross_entropy(start_logits, start) + F.cross_entropy(end_logits, end)\n","\n","        losses.append(loss.item())\n","        del batch, start, end, start_logits, end_logits, loss\n","    dev_losses.append(mean(losses))\n","    print(f\"Evaluation score: {dev_losses[-1]:.3f}\")\n","\n","    model.save_pretrained(f'dump/model.{epoch}')"],"metadata":{"trusted":true,"id":"4lk0X_DiXMyT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","t = list(range(1, 31))\n","plt.plot(t, train_losses, label=\"Train Loss\")\n","plt.plot(t, dev_losses, label=\"Dev Loss\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Loss\")\n","plt.legend()\n","plt.show()\n"],"metadata":{"trusted":true,"id":"yfgTLARbXMyU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["![loss_plot](https://github.com/mynsng/mynsng.github.io/blob/master/assets/images/__results___26_0.png?raw=true)"],"metadata":{"id":"u6mU1rUYXMyV"}},{"cell_type":"markdown","source":["학습 데이터 셋에 Overfitting이 일어나는 것을 확인할 수 있다."],"metadata":{"id":"edkV4LRlXMyV"}},{"cell_type":"markdown","source":["## Answer Inference\n","모델의 Output을 활용해서 질문의 답을 찾는 코드를 작성하자."],"metadata":{"id":"5Nyap_DLXMyW"}},{"cell_type":"code","source":["model = BertForQuestionAnswering.from_pretrained('dump/model.30')\n","model.cuda()\n","model.eval()"],"metadata":{"trusted":true,"id":"0c97VQCGXMyX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for idx, sample in zip(range(1, 4), indexed_train_dataset):\n","    print(f'------{idx}------')\n","    print('Context:', sample['context'])\n","    print('Question:', sample['question'])\n","    \n","    input_ids, token_type_ids = [\n","        torch.tensor(sample[key], dtype=torch.long, device=\"cuda\")\n","        for key in (\"input_ids\", \"token_type_ids\")\n","    ]\n","    \n","    with torch.no_grad():\n","        start_logits, end_logits = model(input_ids=input_ids[None, :], token_type_ids=token_type_ids[None, :])\n","    start_logits.squeeze_(0), end_logits.squeeze_(0)\n","    \n","    start_prob = start_logits[token_type_ids.bool()][1:-1].softmax(-1)\n","    end_prob = end_logits[token_type_ids.bool()][1:-1].softmax(-1)\n","    probability = torch.triu(start_prob[:, None] @ end_prob[None, :])\n","    index = torch.argmax(probability).item()\n","    \n","    start = index // len(end_prob)\n","    end = index % len(end_prob)\n","    \n","    start = sample['position'][start][0]\n","    end = sample['position'][end][1]\n","\n","    print('Answer:', sample['context'][start:end])"],"metadata":{"trusted":true,"id":"Rr3NPhTfXMyX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Test 출력 파일 작성"],"metadata":{"id":"s_EtMnEUXMyY"}},{"cell_type":"code","source":["test_dataset = TokenizedKoMRC.load('/kaggle/input/k-digital-goorm-4-korean-mrc/test.json')\n","test_dataset = IndexerWrappedDataset(test_dataset, indexer)\n","print(\"Number of Test Samples\", len(test_dataset))\n","print(test_dataset[0])"],"metadata":{"trusted":true,"id":"7f4S3Br1XMyZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import csv\n","\n","os.makedirs('out', exist_ok=True)\n","with torch.no_grad(), open('out/baseline.csv', 'w') as fd:\n","    writer = csv.writer(fd)\n","    writer.writerow(['Id', 'Predicted'])\n","\n","    rows = []\n","    for sample in tqdm(test_dataset, \"Testing\"):\n","        input_ids, token_type_ids = [\n","            torch.tensor(sample[key], dtype=torch.long, device=\"cuda\")\n","            for key in (\"input_ids\", \"token_type_ids\")\n","        ]\n","    \n","        with torch.no_grad():\n","            start_logits, end_logits = model(input_ids=input_ids[None, :], token_type_ids=token_type_ids[None, :])\n","        start_logits.squeeze_(0), end_logits.squeeze_(0)\n","    \n","        start_prob = start_logits[token_type_ids.bool()][1:-1].softmax(-1)\n","        end_prob = end_logits[token_type_ids.bool()][1:-1].softmax(-1)\n","        probability = torch.triu(start_prob[:, None] @ end_prob[None, :])\n","        index = torch.argmax(probability).item()\n","    \n","        start = index // len(end_prob)\n","        end = index % len(end_prob)\n","    \n","        start = sample['position'][start][0]\n","        end = sample['position'][end][1]\n","\n","        rows.append([sample[\"guid\"], sample['context'][start:end]])\n","    \n","    writer.writerows(rows)"],"metadata":{"trusted":true,"id":"lBe0O-QWXMya"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"oMaNakA-XMyb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"jA5O3VDRXMyd"},"execution_count":null,"outputs":[]}]}